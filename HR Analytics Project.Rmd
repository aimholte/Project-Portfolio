#HR Analytics Project: Under what circumstances will employees leave a company? What factors predict this?

```{r}
library(readr)
HR_comma_sep <- read_csv("HR_comma_sep.csv")
HR_data <- data.frame(HR_comma_sep)
unique(HR_data$sales)
unique(HR_data$salary)
```
```{r}
hist(HR_data$satisfaction_level)
hist(HR_data$average_montly_hours)
hist(HR_data$time_spend_company)
```
```{r}
plot(sample(HR_data$average_montly_hours, 1000, replace = TRUE),
     sample(HR_data$satisfaction_level, 1000, replace = TRUE))
plot(HR_data$time_spend_company, HR_data$average_montly_hours)
boxplots <- list(boxplot(HR_data$satisfaction_level ~ HR_data$left),
                 boxplot(HR_data$average_montly_hours ~ HR_data$left),
                 boxplot(HR_data$last_evaluation ~ HR_data$left), 
                 boxplot(HR_data$time_spend_company ~ HR_data$left),
                 boxplot(HR_data$number_project ~ HR_data$left))
boxplots
```

```{r}
library(ggplot2)
ggplots <- list(qplot(time_spend_company, satisfaction_level, data = HR_data, color = left, size = salary),
                qplot(average_montly_hours, satisfaction_level, data = HR_data, color = left),
                qplot(number_project, average_montly_hours, data = HR_data, color = left),
                qplot(time_spend_company, satisfaction_level, data = HR_data, color = sales),
                 qplot(time_spend_company, satisfaction_level, data = HR_data,
                       color = salary), qplot(number_project, average_montly_hours,
                                              data = HR_data, color = salary))
print(ggplots)
```

```{r}
correlations <- list(cor(HR_data$satisfaction_level, HR_data$left), cor(HR_data$average_montly_hours, HR_data$left), cor(HR_data$promotion_last_5years, HR_data$left))
summary(HR_data)
correlations
```



#Some More Variables
```{r}
library(dummies)
satisfaction.squared <- HR_data$satisfaction_level^2
satisfaction.cubed <- HR_data$satisfaction_level^3
satisfaction.4 <- HR_data$satisfaction_level^4
satisfaction.5 <- HR_data$satisfaction_level^5
avg_hours.squared <- HR_data$average_montly_hours^2
avg_hours.cubed <- HR_data$average_montly_hours^3
avg_hours.4 <- HR_data$average_montly_hours^4
avg_hours.5 <- HR_data$average_montly_hours^5
time_spent.squared <- HR_data$time_spend_company^2
time_spent.cubed <- HR_data$time_spend_company^3
time_spent.4 <- HR_data$time_spend_company^4
time_spent.5 <- HR_data$time_spend_company^5
project_hours.int <- HR_data$number_project*HR_data$average_montly_hours
satisfaction_time_spent.int <- HR_data$satisfaction_level*HR_data$time_spend_company
HR_data <- data.frame(HR_data, satisfaction.squared, satisfaction.cubed, satisfaction.4, satisfaction.5,
                      avg_hours.squared, avg_hours.cubed, avg_hours.4, avg_hours.5, time_spent.squared,
                      time_spent.squared, time_spent.4, time_spent.5, project_hours.int,
                      satisfaction_time_spent.int)
HR_data <- dummy.data.frame(HR_data)
```
Now that we have added some more predictors to our dataset, let's perform best subset selection to help us find the best model for logistic regression.

```{r}
glm.mod <- glm(left ~.,family = "binomial", data = HR_data)
summary(glm.mod)
```
```{r}
glm.mod.pred <- predict(glm.mod, HR_data, type = "response")
table(HR_data$left, glm.mod.pred > 0.5)
```

#Setting Up the Data for Best Subset Selection
```{r}
step.mod <- step(glm.mod)
summary(step.mod)
```
```{r}
step.formula <- left ~ satisfaction_level + last_evaluation + number_project + 
    average_montly_hours + time_spend_company + Work_accident + 
    promotion_last_5years + salesIT + salesmarketing + salesproduct_mng + 
    salesRandD + salessales + salaryhigh + salarylow + satisfaction.squared + 
    satisfaction.cubed + satisfaction.4 + satisfaction.5 + avg_hours.squared + 
    avg_hours.cubed + avg_hours.4 + avg_hours.5 + time_spent.squared + 
    time_spent.4 + time_spent.5 + project_hours.int + satisfaction_time_spent.int
```

#Cross Validation
```{r}
library(boot)

cost <- function(left, pi = 0) mean(abs(left - pi) > 0.5)
#Randomly shuffle the data
HR_data <- HR_data[sample(nrow(HR_data)),]

#Create 10 equally size folds
folds <- cut(seq(1,nrow(HR_data)),breaks=10,labels=FALSE)

specificity.list <- list()
sensitivity.list <- list()
misclassifcation.rate.list <- list()
#Perform 10 fold cross validation
for(i in 1:10){
    #Segement your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- HR_data[testIndexes, ]
    trainData <- HR_data[-testIndexes, ]
    glm.mod <- glm(step.formula, family = "binomial", data = trainData)
    glm.mod.test <- predict(glm.mod, newdata = testData, type = "response")
    cv.error10 <- cv.glm(testData, glm.mod, cost, K = 10)$delta
    confusion.matrix <- table(testData$left, glm.mod.test > 0.6)
    misclassifcation.rate <- 1 - sum(diag(confusion.matrix))/sum(confusion.matrix)
    confusion.matrix <- as.array(confusion.matrix)
    specificity <- 1 - confusion.matrix[1,2]/rowSums(confusion.matrix)[1]
    sensitivity <- 1 - confusion.matrix[2,1]/rowSums(confusion.matrix)[2]
    specificity.list[i] <- specificity
    sensitivity.list[i] <- sensitivity
    misclassifcation.rate.list[i] <- misclassifcation.rate
}
cv.error10
print(mean(as.numeric(misclassifcation.rate.list)))
print(mean(as.numeric(specificity.list)))
print(mean(as.numeric(sensitivity.list)))
```

