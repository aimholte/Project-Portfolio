#HR Analytics Project: Under what circumstances will employees leave a company? What factors predict this?

```{r}
library(readr)
HR_comma_sep <- read_csv("HR_comma_sep.csv")
HR_data <- data.frame(HR_comma_sep)
unique(HR_data$sales)
unique(HR_data$salary)
```
```{r}
hist(HR_data$satisfaction_level, main = "Distribution of Satisfaction Level at Company")
hist(HR_data$average_montly_hours, main = "Distribution of Average Montly Hours")
hist(HR_data$time_spend_company, main = "Distirbution of Time Spent at Company")
```
```{r}
par(mfrow = c(1, 2))
boxplot(HR_data$satisfaction_level ~ HR_data$left,
        main = "Employee's Satisfaction Level")
boxplot(HR_data$average_montly_hours ~ HR_data$left,
        main = "Hours Worked & Employees Leaving")
par(mfrow = c(1, 2))
boxplot(HR_data$last_evaluation ~ HR_data$left, main = "Time Since Last Evaluation")
boxplot(HR_data$time_spend_company ~ HR_data$left, main = "Time Spent at Company")
par(mfrow = c(1, 2))
boxplot(HR_data$number_project ~ HR_data$left, main = "More Projects, More Leaving?")
boxplot(HR_comma_sep$number_project ~ HR_comma_sep$salary, main = "Number of Projects and Salary")
```

```{r}
library(ggplot2)
library(gridExtra)
attach(HR_data)
attach(HR_comma_sep)
par(mfrow = c(3, 2))
qp.1 <- qplot(time_spend_company, satisfaction_level, data = HR_data, color = left)
qp.2 <- qplot(average_montly_hours, satisfaction_level, data = HR_data, color = left)
qp.3 <- qplot(number_project, average_montly_hours, data = HR_data, color = left)
qp.4 <- qplot(time_spend_company, satisfaction_level, data = HR_data, color = sales)
qp.5 <- qplot(time_spend_company, satisfaction_level, data = HR_data,
                       color = salary)
qp.5 <- qplot(number_project, average_montly_hours, data = HR_data, color = salary)

gg.1 <- ggplot(HR_comma_sep, aes(average_montly_hours, satisfaction_level)) + geom_point(aes(colour = left)) + 
  facet_wrap(~ salary, ncol = 3)
gg.2 <- ggplot(HR_comma_sep, aes(sales, average_montly_hours)) + geom_boxplot() +
  facet_wrap(~ left, ncol = 1)
gg.3 <- ggplot(HR_comma_sep, aes(salary, satisfaction_level)) + geom_boxplot() +
  facet_wrap(~ left, ncol = 1)
gg.4 <- ggplot(HR_comma_sep, aes(x = salary, y = satisfaction_level, fill = left)) +
  geom_bar(stat = "identity", position = "dodge")
qp.2
grid.arrange(qp.1, qp.4)
grid.arrange(qp.3, qp.5)
grid.arrange(gg.1, gg.4)
gg.2
gg.3
```

```{r}
correlations <- list(cor(HR_data$satisfaction_level, HR_data$left), cor(HR_data$average_montly_hours, HR_data$left), cor(HR_data$promotion_last_5years, HR_data$left))
summary(HR_data)
correlations
```



#Some More Variables
```{r}
library(dummies)
satisfaction.squared <- HR_data$satisfaction_level^2
satisfaction.cubed <- HR_data$satisfaction_level^3
satisfaction.4 <- HR_data$satisfaction_level^4
satisfaction.5 <- HR_data$satisfaction_level^5
avg_hours.squared <- HR_data$average_montly_hours^2
avg_hours.cubed <- HR_data$average_montly_hours^3
avg_hours.4 <- HR_data$average_montly_hours^4
avg_hours.5 <- HR_data$average_montly_hours^5
time_spent.squared <- HR_data$time_spend_company^2
time_spent.cubed <- HR_data$time_spend_company^3
time_spent.4 <- HR_data$time_spend_company^4
time_spent.5 <- HR_data$time_spend_company^5
project_hours.int <- HR_data$number_project*HR_data$average_montly_hours
satisfaction_time_spent.int <- HR_data$satisfaction_level*HR_data$time_spend_company
HR_data <- data.frame(HR_data, satisfaction.squared, satisfaction.cubed, satisfaction.4, satisfaction.5,
                      avg_hours.squared, avg_hours.cubed, avg_hours.4, avg_hours.5, time_spent.squared,
                      time_spent.squared, time_spent.4, time_spent.5, project_hours.int,
                      satisfaction_time_spent.int)
HR_data <- dummy.data.frame(HR_data)
```
Now that we have added some more predictors to our dataset, let's perform best subset selection to help us find the best model for logistic regression.

```{r}
glm.mod <- glm(left ~.,family = "binomial", data = HR_data)
summary(glm.mod)
```
```{r}
glm.mod.pred <- predict(glm.mod, HR_data, type = "response")
table(HR_data$left, glm.mod.pred > 0.5)
```

#Setting Up the Data for Best Subset Selection
```{r}
#step.mod <- step(glm.mod)
#summary(step.mod)
```
```{r}
step.formula <- left ~ satisfaction_level + last_evaluation + number_project + 
    average_montly_hours + time_spend_company + Work_accident + 
    promotion_last_5years + salesIT + salesmarketing + salesproduct_mng + 
    salesRandD + salessales + salaryhigh + salarylow + satisfaction.squared + 
    satisfaction.cubed + satisfaction.4 + satisfaction.5 + avg_hours.squared + 
    avg_hours.cubed + avg_hours.4 + avg_hours.5 + time_spent.squared + 
    time_spent.4 + time_spent.5 + project_hours.int + satisfaction_time_spent.int
```

#Cross Validation

```{r}
library(boot)

cost <- function(left, pi = 0) mean(abs(left - pi) > 0.5)
#Randomly shuffle the data
HR_data <- HR_data[sample(nrow(HR_data)),]

#Create 10 equally size folds
folds <- cut(seq(1,nrow(HR_data)),breaks=10,labels=FALSE)

specificity.list <- list()
sensitivity.list <- list()
misclassifcation.rate.list <- list()
#Perform 10 fold cross validation
for(i in 1:10){
    #Segement your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- HR_data[testIndexes, ]
    trainData <- HR_data[-testIndexes, ]
    glm.mod <- glm(step.formula, family = "binomial", data = trainData)
    glm.mod.test <- predict(glm.mod, newdata = testData, type = "response")
    cv.error10 <- cv.glm(testData, glm.mod, cost, K = 10)$delta
    confusion.matrix <- table(testData$left, glm.mod.test > 0.5)
    misclassifcation.rate <- 1 - sum(diag(confusion.matrix))/sum(confusion.matrix)
    confusion.matrix <- as.array(confusion.matrix)
    specificity <- 1 - confusion.matrix[1,2]/rowSums(confusion.matrix)[1]
    sensitivity <- 1 - confusion.matrix[2,1]/rowSums(confusion.matrix)[2]
    specificity.list[i] <- specificity
    sensitivity.list[i] <- sensitivity
    misclassifcation.rate.list[i] <- misclassifcation.rate
}
```

```{r}
print(mean(as.numeric(misclassifcation.rate.list)))
print(mean(as.numeric(specificity.list)))
print(mean(as.numeric(sensitivity.list)))
```
```{r}
library(rpart)
library(rpart.plot)
library(RColorBrewer)
tree.class <- rpart(left ~ ., method = "class", data = HR_data)
rpart.plot(tree.class)
printcp(tree.class)
plotcp(tree.class)
table(HR_data$left, tree.class$y) #Looks like overfitting.
pruned.tree <- prune(tree.class, cp = tree.class$cptable[which.min(tree.class$cptable[,"xerror"]),"CP"])
rpart.plot(pruned.tree)
```

